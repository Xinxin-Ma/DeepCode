"""
Human Study Simulation for ACB-Agent Explanation Quality Evaluation

This module implements simulated human studies to evaluate the quality and interpretability
of explanations generated by ACB-Agent systems. It simulates human participants with
different expertise levels and measures explanation effectiveness.

Based on the paper's methodology for evaluating human-AI interaction quality.
"""

import torch
import numpy as np
import json
import logging
import time
import random
from typing import Dict, List, Tuple, Optional, Union, Any
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict, Counter
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Import our ACB-Agent components
from ..models.acb_agent import ACBAgent, MultiAgentACBSystem
from ..utils.concept_utils import ConceptVocabulary
from ..utils.explanation_generator import ExplanationGenerator, ExplanationStyle
from ..environments.human_interaction import HumanInteractionEnvironment, InteractionMode


class ParticipantExpertise(Enum):
    """Simulated participant expertise levels"""
    NOVICE = "novice"
    INTERMEDIATE = "intermediate"
    EXPERT = "expert"


class StudyTask(Enum):
    """Types of human study tasks"""
    EXPLANATION_QUALITY = "explanation_quality"
    CONCEPT_ALIGNMENT = "concept_alignment"
    TRUST_CALIBRATION = "trust_calibration"
    DECISION_SUPPORT = "decision_support"
    COMPARATIVE_EVALUATION = "comparative_evaluation"


@dataclass
class ParticipantProfile:
    """Simulated human participant profile"""
    participant_id: str
    expertise: ParticipantExpertise
    domain_knowledge: float  # 0.0 to 1.0
    concept_familiarity: Dict[str, float] = field(default_factory=dict)
    response_consistency: float = 0.8  # How consistent their responses are
    attention_level: float = 0.9  # How carefully they evaluate
    bias_factors: Dict[str, float] = field(default_factory=dict)


@dataclass
class StudyConfiguration:
    """Configuration for human study simulation"""
    study_name: str
    task_type: StudyTask
    num_participants: int = 50
    num_trials_per_participant: int = 20
    expertise_distribution: Dict[ParticipantExpertise, float] = field(
        default_factory=lambda: {
            ParticipantExpertise.NOVICE: 0.4,
            ParticipantExpertise.INTERMEDIATE: 0.4,
            ParticipantExpertise.EXPERT: 0.2
        }
    )
    randomize_order: bool = True
    include_baseline_comparison: bool = True
    collect_qualitative_feedback: bool = True
    time_limit_per_trial: float = 30.0  # seconds
    confidence_rating_scale: int = 5  # 1-5 scale


class HumanStudySimulator:
    """
    Simulates human studies for evaluating ACB-Agent explanation quality
    """
    
    def __init__(
        self,
        agent_system: Union[ACBAgent, MultiAgentACBSystem],
        concept_vocab: ConceptVocabulary,
        config: StudyConfiguration,
        device: str = "cpu"
    ):
        self.agent_system = agent_system
        self.concept_vocab = concept_vocab
        self.config = config
        self.device = device
        
        # Initialize interaction environment
        self.interaction_env = HumanInteractionEnvironment(
            agent_system=agent_system,
            concept_vocab=concept_vocab,
            interaction_mode=InteractionMode.EXPLANATION_FEEDBACK,
            k_concepts=3
        )
        
        # Generate participant profiles
        self.participants = self._generate_participant_profiles()
        
        # Study results storage
        self.study_results = {
            'participant_responses': [],
            'trial_data': [],
            'aggregate_metrics': {},
            'qualitative_feedback': []
        }
        
        # Logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
    def _generate_participant_profiles(self) -> List[ParticipantProfile]:
        """Generate simulated participant profiles"""
        participants = []
        
        for i in range(self.config.num_participants):
            # Sample expertise level based on distribution
            expertise_probs = list(self.config.expertise_distribution.values())
            expertise_levels = list(self.config.expertise_distribution.keys())
            expertise = np.random.choice(expertise_levels, p=expertise_probs)
            
            # Generate domain knowledge based on expertise
            if expertise == ParticipantExpertise.NOVICE:
                domain_knowledge = np.random.beta(2, 5)  # Skewed towards lower values
            elif expertise == ParticipantExpertise.INTERMEDIATE:
                domain_knowledge = np.random.beta(3, 3)  # More balanced
            else:  # EXPERT
                domain_knowledge = np.random.beta(5, 2)  # Skewed towards higher values
            
            # Generate concept familiarity
            concept_familiarity = {}
            for concept_name in self.concept_vocab.concept_names:
                base_familiarity = domain_knowledge
                # Add some noise
                familiarity = np.clip(
                    base_familiarity + np.random.normal(0, 0.1),
                    0.0, 1.0
                )
                concept_familiarity[concept_name] = familiarity
            
            # Generate other characteristics
            response_consistency = np.random.beta(8, 2)  # Generally high consistency
            attention_level = np.random.beta(7, 3)  # Generally high attention
            
            participant = ParticipantProfile(
                participant_id=f"P{i+1:03d}",
                expertise=expertise,
                domain_knowledge=domain_knowledge,
                concept_familiarity=concept_familiarity,
                response_consistency=response_consistency,
                attention_level=attention_level
            )
            
            participants.append(participant)
            
        return participants
    
    def _simulate_participant_response(
        self,
        participant: ParticipantProfile,
        explanation: str,
        top_concepts: List[Tuple[str, float]],
        action: str,
        state_info: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        Simulate how a participant would respond to an explanation
        """
        # Base quality score influenced by participant characteristics
        base_quality = 0.5
        
        # Adjust based on concept familiarity
        concept_familiarity_bonus = 0.0
        for concept_name, activation in top_concepts:
            if concept_name in participant.concept_familiarity:
                familiarity = participant.concept_familiarity[concept_name]
                concept_familiarity_bonus += familiarity * activation * 0.1
        
        # Adjust based on domain knowledge
        domain_bonus = participant.domain_knowledge * 0.2
        
        # Adjust based on explanation quality (simulated)
        explanation_length_factor = min(len(explanation.split()) / 20.0, 1.0)
        explanation_bonus = explanation_length_factor * 0.1
        
        # Calculate perceived quality
        perceived_quality = base_quality + concept_familiarity_bonus + domain_bonus + explanation_bonus
        
        # Add noise based on response consistency
        noise_std = (1.0 - participant.response_consistency) * 0.2
        perceived_quality += np.random.normal(0, noise_std)
        
        # Clip to valid range
        perceived_quality = np.clip(perceived_quality, 0.0, 1.0)
        
        # Convert to rating scale
        quality_rating = int(perceived_quality * 5) + 1  # 1-6 scale
        quality_rating = np.clip(quality_rating, 1, 5)
        
        # Simulate confidence rating
        confidence_base = participant.attention_level * participant.domain_knowledge
        confidence_noise = np.random.normal(0, 0.1)
        confidence = np.clip(confidence_base + confidence_noise, 0.0, 1.0)
        confidence_rating = int(confidence * self.config.confidence_rating_scale) + 1
        
        # Simulate response time
        base_time = 10.0  # Base response time in seconds
        expertise_factor = {
            ParticipantExpertise.EXPERT: 0.8,
            ParticipantExpertise.INTERMEDIATE: 1.0,
            ParticipantExpertise.NOVICE: 1.3
        }[participant.expertise]
        
        response_time = base_time * expertise_factor * np.random.lognormal(0, 0.3)
        response_time = min(response_time, self.config.time_limit_per_trial)
        
        # Generate qualitative feedback (simplified)
        feedback_templates = [
            "The explanation was clear and helpful.",
            "I understood the main concepts mentioned.",
            "The explanation could be more detailed.",
            "The concepts were relevant to the action.",
            "I would trust this explanation in practice."
        ]
        
        qualitative_feedback = ""
        if self.config.collect_qualitative_feedback and np.random.random() < 0.3:
            qualitative_feedback = np.random.choice(feedback_templates)
        
        return {
            'participant_id': participant.participant_id,
            'quality_rating': quality_rating,
            'confidence_rating': confidence_rating,
            'response_time': response_time,
            'perceived_quality': perceived_quality,
            'qualitative_feedback': qualitative_feedback,
            'concept_familiarity_scores': {
                concept: participant.concept_familiarity.get(concept, 0.0)
                for concept, _ in top_concepts
            }
        }
    
    def run_explanation_quality_study(
        self,
        test_states: torch.Tensor,
        ground_truth_actions: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Run explanation quality evaluation study
        """
        self.logger.info(f"Starting explanation quality study with {len(self.participants)} participants")
        
        study_results = {
            'participant_responses': [],
            'trial_data': [],
            'summary_statistics': {}
        }
        
        # Generate test scenarios
        num_trials = min(len(test_states), self.config.num_trials_per_participant)
        
        for participant in self.participants:
            participant_responses = []
            
            # Randomize trial order if configured
            trial_indices = list(range(num_trials))
            if self.config.randomize_order:
                np.random.shuffle(trial_indices)
            
            for trial_idx in trial_indices:
                state = test_states[trial_idx].unsqueeze(0)
                
                # Get agent action and explanation
                with torch.no_grad():
                    result = self.agent_system.act(
                        state, 
                        deterministic=True, 
                        return_explanation=True
                    )
                
                action = result['actions'][0] if isinstance(result['actions'], list) else result['actions']
                explanation = result.get('explanation', "No explanation available")
                top_concepts = result.get('top_concepts', [])
                
                # Convert action to string if needed
                if isinstance(action, torch.Tensor):
                    action_str = f"Action_{action.item()}"
                else:
                    action_str = str(action)
                
                # Simulate participant response
                response = self._simulate_participant_response(
                    participant=participant,
                    explanation=explanation,
                    top_concepts=top_concepts,
                    action=action_str,
                    state_info={'trial_idx': trial_idx}
                )
                
                # Add trial information
                trial_data = {
                    'trial_idx': trial_idx,
                    'state_tensor': state.cpu().numpy(),
                    'action': action_str,
                    'explanation': explanation,
                    'top_concepts': top_concepts,
                    'participant_response': response
                }
                
                participant_responses.append(response)
                study_results['trial_data'].append(trial_data)
            
            study_results['participant_responses'].extend(participant_responses)
        
        # Compute summary statistics
        study_results['summary_statistics'] = self._compute_study_statistics(
            study_results['participant_responses']
        )
        
        self.study_results = study_results
        return study_results
    
    def run_concept_alignment_study(
        self,
        test_states: torch.Tensor,
        ground_truth_concepts: List[List[str]]
    ) -> Dict[str, Any]:
        """
        Run concept alignment evaluation study
        """
        self.logger.info("Starting concept alignment study")
        
        alignment_results = {
            'alignment_scores': [],
            'participant_concept_ratings': [],
            'concept_recognition_accuracy': {}
        }
        
        num_trials = min(len(test_states), len(ground_truth_concepts))
        
        for participant in self.participants:
            for trial_idx in range(num_trials):
                state = test_states[trial_idx].unsqueeze(0)
                true_concepts = ground_truth_concepts[trial_idx]
                
                # Get agent's concept activations
                with torch.no_grad():
                    result = self.agent_system.act(state, return_explanation=True)
                
                top_concepts = result.get('top_concepts', [])
                predicted_concepts = [concept for concept, _ in top_concepts]
                
                # Simulate participant's concept recognition
                recognized_concepts = []
                for concept in predicted_concepts:
                    familiarity = participant.concept_familiarity.get(concept, 0.0)
                    recognition_prob = familiarity * participant.attention_level
                    
                    if np.random.random() < recognition_prob:
                        recognized_concepts.append(concept)
                
                # Compute alignment score
                if true_concepts:
                    alignment_score = len(set(recognized_concepts) & set(true_concepts)) / len(true_concepts)
                else:
                    alignment_score = 0.0
                
                alignment_results['alignment_scores'].append({
                    'participant_id': participant.participant_id,
                    'trial_idx': trial_idx,
                    'alignment_score': alignment_score,
                    'recognized_concepts': recognized_concepts,
                    'true_concepts': true_concepts
                })
        
        return alignment_results
    
    def run_comparative_study(
        self,
        test_states: torch.Tensor,
        baseline_explanations: List[str]
    ) -> Dict[str, Any]:
        """
        Run comparative study between ACB-Agent and baseline explanations
        """
        self.logger.info("Starting comparative evaluation study")
        
        comparative_results = {
            'preference_scores': [],
            'quality_comparisons': [],
            'statistical_tests': {}
        }
        
        num_trials = min(len(test_states), len(baseline_explanations))
        
        for participant in self.participants:
            for trial_idx in range(num_trials):
                state = test_states[trial_idx].unsqueeze(0)
                baseline_explanation = baseline_explanations[trial_idx]
                
                # Get ACB-Agent explanation
                with torch.no_grad():
                    result = self.agent_system.act(state, return_explanation=True)
                
                acb_explanation = result.get('explanation', "No explanation available")
                top_concepts = result.get('top_concepts', [])
                
                # Simulate participant responses to both explanations
                acb_response = self._simulate_participant_response(
                    participant, acb_explanation, top_concepts, "action"
                )
                
                baseline_response = self._simulate_participant_response(
                    participant, baseline_explanation, [], "action"
                )
                
                # Determine preference
                quality_diff = acb_response['perceived_quality'] - baseline_response['perceived_quality']
                preference = "acb" if quality_diff > 0.1 else "baseline" if quality_diff < -0.1 else "neutral"
                
                comparative_results['preference_scores'].append({
                    'participant_id': participant.participant_id,
                    'trial_idx': trial_idx,
                    'preference': preference,
                    'acb_quality': acb_response['quality_rating'],
                    'baseline_quality': baseline_response['quality_rating'],
                    'quality_difference': quality_diff
                })
        
        # Perform statistical tests
        acb_ratings = [r['acb_quality'] for r in comparative_results['preference_scores']]
        baseline_ratings = [r['baseline_quality'] for r in comparative_results['preference_scores']]
        
        # Wilcoxon signed-rank test
        statistic, p_value = stats.wilcoxon(acb_ratings, baseline_ratings)
        
        comparative_results['statistical_tests'] = {
            'wilcoxon_statistic': statistic,
            'wilcoxon_p_value': p_value,
            'mean_acb_rating': np.mean(acb_ratings),
            'mean_baseline_rating': np.mean(baseline_ratings),
            'effect_size': (np.mean(acb_ratings) - np.mean(baseline_ratings)) / np.std(acb_ratings + baseline_ratings)
        }
        
        return comparative_results
    
    def _compute_study_statistics(self, responses: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Compute summary statistics for study results"""
        
        quality_ratings = [r['quality_rating'] for r in responses]
        confidence_ratings = [r['confidence_rating'] for r in responses]
        response_times = [r['response_time'] for r in responses]
        
        # Group by participant expertise
        expertise_groups = defaultdict(list)
        for response in responses:
            participant = next(p for p in self.participants if p.participant_id == response['participant_id'])
            expertise_groups[participant.expertise.value].append(response)
        
        expertise_stats = {}
        for expertise, group_responses in expertise_groups.items():
            group_quality = [r['quality_rating'] for r in group_responses]
            expertise_stats[expertise] = {
                'mean_quality': np.mean(group_quality),
                'std_quality': np.std(group_quality),
                'count': len(group_responses)
            }
        
        return {
            'overall_statistics': {
                'mean_quality_rating': np.mean(quality_ratings),
                'std_quality_rating': np.std(quality_ratings),
                'mean_confidence_rating': np.mean(confidence_ratings),
                'std_confidence_rating': np.std(confidence_ratings),
                'mean_response_time': np.mean(response_times),
                'std_response_time': np.std(response_times),
                'total_responses': len(responses)
            },
            'expertise_breakdown': expertise_stats,
            'quality_distribution': dict(Counter(quality_ratings)),
            'confidence_distribution': dict(Counter(confidence_ratings))
        }
    
    def generate_study_report(self, save_path: Optional[str] = None) -> str:
        """Generate comprehensive study report"""
        
        if not self.study_results:
            return "No study results available. Run a study first."
        
        report_lines = [
            f"# Human Study Simulation Report",
            f"## Study Configuration",
            f"- Study Name: {self.config.study_name}",
            f"- Task Type: {self.config.task_type.value}",
            f"- Number of Participants: {self.config.num_participants}",
            f"- Trials per Participant: {self.config.num_trials_per_participant}",
            "",
            f"## Overall Results"
        ]
        
        if 'summary_statistics' in self.study_results:
            stats = self.study_results['summary_statistics']['overall_statistics']
            report_lines.extend([
                f"- Mean Quality Rating: {stats['mean_quality_rating']:.2f} ± {stats['std_quality_rating']:.2f}",
                f"- Mean Confidence Rating: {stats['mean_confidence_rating']:.2f} ± {stats['std_confidence_rating']:.2f}",
                f"- Mean Response Time: {stats['mean_response_time']:.2f} ± {stats['std_response_time']:.2f} seconds",
                f"- Total Responses: {stats['total_responses']}",
                ""
            ])
            
            # Expertise breakdown
            report_lines.append("## Results by Expertise Level")
            expertise_stats = self.study_results['summary_statistics']['expertise_breakdown']
            for expertise, stats in expertise_stats.items():
                report_lines.append(
                    f"- {expertise.title()}: {stats['mean_quality']:.2f} ± {stats['std_quality']:.2f} "
                    f"(n={stats['count']})"
                )
        
        report_text = "\n".join(report_lines)
        
        if save_path:
            with open(save_path, 'w') as f:
                f.write(report_text)
            self.logger.info(f"Study report saved to {save_path}")
        
        return report_text
    
    def visualize_results(self, save_dir: Optional[str] = None):
        """Create visualizations of study results"""
        
        if not self.study_results or 'summary_statistics' not in self.study_results:
            self.logger.warning("No study results available for visualization")
            return
        
        # Set up plotting style
        plt.style.use('default')
        sns.set_palette("husl")
        
        # Create figure with subplots
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle(f'Human Study Results: {self.config.study_name}', fontsize=16)
        
        # Plot 1: Quality rating distribution
        quality_ratings = [r['quality_rating'] for r in self.study_results['participant_responses']]
        axes[0, 0].hist(quality_ratings, bins=range(1, 7), alpha=0.7, edgecolor='black')
        axes[0, 0].set_title('Quality Rating Distribution')
        axes[0, 0].set_xlabel('Quality Rating')
        axes[0, 0].set_ylabel('Frequency')
        
        # Plot 2: Quality by expertise level
        expertise_data = []
        expertise_labels = []
        for response in self.study_results['participant_responses']:
            participant = next(p for p in self.participants if p.participant_id == response['participant_id'])
            expertise_data.append(response['quality_rating'])
            expertise_labels.append(participant.expertise.value)
        
        expertise_df = {'Quality': expertise_data, 'Expertise': expertise_labels}
        sns.boxplot(data=expertise_df, x='Expertise', y='Quality', ax=axes[0, 1])
        axes[0, 1].set_title('Quality Ratings by Expertise Level')
        
        # Plot 3: Response time distribution
        response_times = [r['response_time'] for r in self.study_results['participant_responses']]
        axes[1, 0].hist(response_times, bins=20, alpha=0.7, edgecolor='black')
        axes[1, 0].set_title('Response Time Distribution')
        axes[1, 0].set_xlabel('Response Time (seconds)')
        axes[1, 0].set_ylabel('Frequency')
        
        # Plot 4: Confidence vs Quality scatter
        confidence_ratings = [r['confidence_rating'] for r in self.study_results['participant_responses']]
        axes[1, 1].scatter(confidence_ratings, quality_ratings, alpha=0.6)
        axes[1, 1].set_title('Confidence vs Quality Ratings')
        axes[1, 1].set_xlabel('Confidence Rating')
        axes[1, 1].set_ylabel('Quality Rating')
        
        # Add correlation coefficient
        correlation = np.corrcoef(confidence_ratings, quality_ratings)[0, 1]
        axes[1, 1].text(0.05, 0.95, f'r = {correlation:.3f}', transform=axes[1, 1].transAxes)
        
        plt.tight_layout()
        
        if save_dir:
            plt.savefig(f"{save_dir}/human_study_results.png", dpi=300, bbox_inches='tight')
            self.logger.info(f"Visualization saved to {save_dir}/human_study_results.png")
        
        plt.show()


def create_human_study_simulator(
    agent_system: Union[ACBAgent, MultiAgentACBSystem],
    concept_vocab: ConceptVocabulary,
    study_name: str = "ACB_Agent_Evaluation",
    task_type: StudyTask = StudyTask.EXPLANATION_QUALITY,
    num_participants: int = 50,
    num_trials: int = 20
) -> HumanStudySimulator:
    """
    Factory function to create a configured human study simulator
    """
    config = StudyConfiguration(
        study_name=study_name,
        task_type=task_type,
        num_participants=num_participants,
        num_trials_per_participant=num_trials
    )
    
    return HumanStudySimulator(
        agent_system=agent_system,
        concept_vocab=concept_vocab,
        config=config
    )


def run_comprehensive_human_study(
    agent_system: Union[ACBAgent, MultiAgentACBSystem],
    concept_vocab: ConceptVocabulary,
    test_states: torch.Tensor,
    ground_truth_concepts: Optional[List[List[str]]] = None,
    baseline_explanations: Optional[List[str]] = None,
    save_dir: Optional[str] = None
) -> Dict[str, Any]:
    """
    Run a comprehensive human study evaluation
    """
    results = {}
    
    # Explanation Quality Study
    quality_simulator = create_human_study_simulator(
        agent_system, concept_vocab, "Quality_Evaluation", StudyTask.EXPLANATION_QUALITY
    )
    results['quality_study'] = quality_simulator.run_explanation_quality_study(test_states)
    
    # Concept Alignment Study (if ground truth available)
    if ground_truth_concepts:
        alignment_simulator = create_human_study_simulator(
            agent_system, concept_vocab, "Concept_Alignment", StudyTask.CONCEPT_ALIGNMENT
        )
        results['alignment_study'] = alignment_simulator.run_concept_alignment_study(
            test_states, ground_truth_concepts
        )
    
    # Comparative Study (if baseline available)
    if baseline_explanations:
        comparative_simulator = create_human_study_simulator(
            agent_system, concept_vocab, "Comparative_Evaluation", StudyTask.COMPARATIVE_EVALUATION
        )
        results['comparative_study'] = comparative_simulator.run_comparative_study(
            test_states, baseline_explanations
        )
    
    # Generate reports and visualizations
    if save_dir:
        for study_name, study_results in results.items():
            if study_name == 'quality_study':
                quality_simulator.study_results = study_results
                report = quality_simulator.generate_study_report(f"{save_dir}/{study_name}_report.md")
                quality_simulator.visualize_results(save_dir)
    
    return results


if __name__ == "__main__":
    # Example usage
    print("Human Study Simulation Module")
    print("This module provides comprehensive human study simulation capabilities")
    print("for evaluating ACB-Agent explanation quality and interpretability.")